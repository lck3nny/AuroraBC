{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eacf32c",
   "metadata": {},
   "source": [
    "# Scraper 3.2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "b1498d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os, io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from pathlib import Path  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from windrose import WindroseAxes\n",
    "from PIL import Image\n",
    "from google.cloud import storage\n",
    "import google.cloud.logging\n",
    "import logging\n",
    "from flask import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "19a595b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alive_progress import alive_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "11f4a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../service_account.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "7b253241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# LOGGING SETUP\n",
    "# -------------------------------------------------------------------------------\n",
    "client = google.cloud.logging.Client()\n",
    "client.setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "bb2110c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# CREATE DATERANGE\n",
    "# -------------------------------------------------------------------------------\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cd991",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "---\n",
    "Import existing weather data from GoogleCloudStorage as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "d5f2f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# IMPORT DATASET\n",
    "# Function to import all weather data\n",
    "# Data Patching completed in this step\n",
    "# -------------------------------------------------------------------------------\n",
    "def import_weather_data():\n",
    "    # Connect to Google Cloud Storage\n",
    "    # -------------------------------\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Open Bucket\n",
    "    # -----------\n",
    "    bucket_name = 'weather_aurorabc'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Open Blob\n",
    "    # ---------\n",
    "    blob_name = 'weather_ampm.csv'\n",
    "    #blob_name = 'backups/weather-2023-09-26.csv'\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Read Weather Data from Blob\n",
    "    # ---------------------------\n",
    "    with blob.open(\"r\") as f:\n",
    "        weather_data = pd.read_csv(f)\n",
    "\n",
    "    # Cast all data variables to float\n",
    "    weather_data = weather_data.astype(\n",
    "        {'Height':'float',\n",
    "        'Temp':'float',\n",
    "        'Dew_Point':'float',\n",
    "        'Relative_Humidity':'float',\n",
    "        'Mixing_Ratio':'float',\n",
    "        'Wind_Direction':'float',\n",
    "        'Wind_Speed':'float',\n",
    "        'Potential_Temp':'float',\n",
    "        'Equivalent_Potential_Temp':'float',\n",
    "        'Virtual_Potential_Temp':'float',\n",
    "        })\n",
    "\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1d913",
   "metadata": {},
   "source": [
    "# Find Missing Values\n",
    "---\n",
    "Look through existing dataset and create list of missing data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "6bfd2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# FIND MISSING VALS\n",
    "# Function to find incomplete or missing vals within a dataset\n",
    "# -------------------------------------------------------------------------------\n",
    "def find_missing_vals(weather_data, start_date = date(2020, 1, 1), end_date = datetime.today().date()):\n",
    "    missing_data = []\n",
    "    stations = [\n",
    "        {\"id\": 72797, \"name\": \"Quillayute\"},\n",
    "        {\"id\": 73033, \"name\": \"Vernon\"},\n",
    "        {\"id\": 71109, \"name\": \"Port Hardy\"}]\n",
    "    pressures = [700, 850]\n",
    "    dates = daterange(start_date, end_date)\n",
    "    \n",
    "    # Loop through all dates from Jan 1st 2020 until today\n",
    "    # For each date in the range, find the station / pressure / time combinations that are missing\n",
    "    for i, d in enumerate(dates):\n",
    "        missing_data.append({\n",
    "            'date': str(d),\n",
    "            'stations': []\n",
    "        })\n",
    "        for j, s in enumerate(stations):\n",
    "            # Create empty array of pressures for each station at this date value\n",
    "            for p in pressures:\n",
    "                am = weather_data.loc[(weather_data['Date'] == str(d)) & (weather_data['Station'] == str(s['name'])) & (weather_data['Pressure'] == int(p)) & (weather_data['Time'] == 'AM')]\n",
    "                pm = weather_data.loc[(weather_data['Date'] == str(d)) & (weather_data['Station'] == str(s['name'])) & (weather_data['Pressure'] == int(p)) & (weather_data['Time'] == 'PM')]\n",
    "                \n",
    "                \n",
    "                # If there is no record in the dataset for this date...\n",
    "                if am.empty or pm.empty:\n",
    "                    missing_data[i]['stations'].append({'id': s['id'], 'name': s['name'], 'pressures': []}) \n",
    "                    missing_data[i]['stations'][-1]['pressures'].append(str(p))\n",
    "       \n",
    "\n",
    "                    continue\n",
    "                \n",
    "                # If there are null values for Temp or WindSpeed...\n",
    "                if not am['Temp'].any() or not am['Wind_Speed'].any() or not pm['Temp'].any() or not pm['Wind_Speed'].any():\n",
    "                    missing_data[i]['stations'].append({'id': s['id'], 'name': s['name'], 'pressures': []}) \n",
    "                    missing_data[i]['stations'][-1]['pressures'].append(str(p))\n",
    "      \n",
    "    \n",
    "    \n",
    "    # Remove unecessary rows where no new data is required\n",
    "    missing_data = [x for x in missing_data if x['stations']]\n",
    "\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d4830",
   "metadata": {},
   "source": [
    "# Scrape Data\n",
    "---\n",
    "Using a list of incomplete data, generate a list of URLs and scrape. \n",
    "\n",
    "Return new Dataset in the format of a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "89caa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# SCRAPE DATA\n",
    "# Itteratively scrape weather balloon data from UWYO site\n",
    "# Extract 700 and 850 rows from both AM and PM readings\n",
    "# Return data in Pandas DF\n",
    "# -------------------------------------------------------------------------------\n",
    "def scrape_data(missing_vals):\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # GENERATE LIST OF URLS TO BE SCRAPED\n",
    "    print(\"Creating list of URLs to be scraped\")\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    base_url = \"http://weather.uwyo.edu/cgi-bin/sounding?region=naconf&TYPE=TEXT%3ALIST\"\n",
    "    times = {'AM': [0, 11], 'PM': [12, 23]}\n",
    "    \n",
    "    '''\n",
    "    x = {\n",
    "        'date': '2020-01-01', \n",
    "        'stations': [\n",
    "            {'id': 72797, 'name': 'Quillayute', 'pressures': ['700']}, \n",
    "            {'id': 73033, 'name': 'Vernon', 'pressures': ['850']}, \n",
    "            {'id': 71109, 'name': 'Port Hardy', 'pressures': []}]\n",
    "        }\n",
    "    '''\n",
    "    \n",
    "    print(\"Creating dates list...\")\n",
    "\n",
    "    # Create list of URLs for each station\n",
    "    # ------------------------------------\n",
    "    urls = []\n",
    "    dates_idx = []\n",
    "    station_idx = []\n",
    "    for val in missing_vals:\n",
    "        d = val['date'].split('-')\n",
    "        for station in val['stations']:\n",
    "            for t, time in times.items():\n",
    "                url = base_url\n",
    "                url += \"&YEAR={}\".format(d[0])\n",
    "                url += \"&MONTH={}\".format(d[1])\n",
    "                url += \"&FROM={:0>2d}{:0>2d}\".format(int(d[2]), time[0])\n",
    "                url += \"&TO={:0>2d}{:0>2d}\".format(int(d[2]), time[1])\n",
    "                url += \"&STNM={}\".format(station[\"id\"])\n",
    "                url += \"&REPLOT=1\"\n",
    "                urls.append(url)\n",
    "\n",
    "                dates_idx.append(\"%02d-%02d-%02d\" % (int(d[0]), int(d[1]), int(d[2])))\n",
    "                station_idx.append(station[\"name\"])\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # EXTRACT DATA FROM URLS\n",
    "    print(\"Extracting data from list of URLs\")\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #print(\"Scraping URLS...\")\n",
    "    #print(urls)\n",
    "\n",
    "    retry = 1\n",
    "    data = []\n",
    "    ampm = 0\n",
    "    problem_urls = []\n",
    "    with alive_bar(len(urls), force_tty=True) as bar:\n",
    "        for i, url in enumerate(urls):\n",
    "            search = 0\n",
    "            while search >= 0:\n",
    "                # Scrape site data\n",
    "                # ~~~~~~~~~~~~~~~~\n",
    "                search += 1\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                results = str(soup.find('pre'))\n",
    "                sevhundy = results[results.find(\"700\"):].split()[:11]\n",
    "                eightfiddy = results[results.find(\"850\"):].split()[:11]\n",
    "                \n",
    "\n",
    "                # Catch erroneous reads\n",
    "                # Site can hit too many requests\n",
    "                # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                if 'e' not in sevhundy[0] and 'e' not in eightfiddy[0]:\n",
    "                    search = -1\n",
    "                # Try 3 times then move on\n",
    "                elif search == retry:\n",
    "                    print(\"Could not extract data from: {}\".format(url))\n",
    "                    problem_urls.append(url)\n",
    "                    search = -1\n",
    "\n",
    "            # Extract 700 first // 850 second\n",
    "            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            data.append(sevhundy)\n",
    "            data[-1].insert(0, dates_idx[i])\n",
    "            data[-1].insert(1, station_idx[i])\n",
    "            data[-1].insert(2, list(times.keys())[ampm%2])\n",
    "\n",
    "            data.append(eightfiddy)\n",
    "            data[-1].insert(0, dates_idx[i])\n",
    "            data[-1].insert(1, station_idx[i])\n",
    "            data[-1].insert(2, list(times.keys())[ampm%2])\n",
    "            \n",
    "            ampm += 1\n",
    "                        \n",
    "            bar()\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # CLEAN EXTRACTED DATA\n",
    "    print(\"Cleaning extracted data\")\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    # Define data cols\n",
    "    # ----------------\n",
    "    cols = [\n",
    "        \"Date\", \n",
    "        \"Station\", \n",
    "        \"Time\",\n",
    "        \"Pressure\", \n",
    "        \"Height\", \n",
    "        \"Temp\", \n",
    "        \"Dew_Point\", \n",
    "        \"Relative_Humidity\", \n",
    "        \"Mixing_Ratio\", \n",
    "        \"Wind_Direction\", \n",
    "        \"Wind_Speed\", \n",
    "        \"Potential_Temp\",\n",
    "        \"Equivalent_Potential_Temp\",\n",
    "        \"Virtual_Potential_Temp\"]\n",
    "\n",
    "    # Remove incomplete rows from dataset\n",
    "    # -----------------------------------\n",
    "    data = [row for row in data if len(row) >= len(cols)]\n",
    "\n",
    "    # Remove rogue \"e\" values from pressure field\n",
    "    # -------------------------------------------\n",
    "    pressures = [700, 850]\n",
    "    ctr = 0\n",
    "    for item in data:\n",
    "        if(not str(item[3]).isnumeric()):\n",
    "            item[3] = pressures[ctr%2]\n",
    "        ctr += 1\n",
    "\n",
    "    # Convert data to Pandas DataFrame\n",
    "    # --------------------------------\n",
    "    new_data = pd.DataFrame(data, columns=cols)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ea4b8",
   "metadata": {},
   "source": [
    "# Standardize Datatypes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "c5954e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# STANDARDIZE DATATYPES\n",
    "# Ensrues each col follows correct data type\n",
    "# -------------------------------------------------------------------------------\n",
    "def standardize_dtypes(data):\n",
    "    return data.astype( dtype={\n",
    "                'Date' : str, \n",
    "                'Station': str,\n",
    "                'Time': str,\n",
    "                'Pressure': int,\n",
    "                'Height': float,\n",
    "                'Temp': float,\n",
    "                'Dew_Point': float,\n",
    "                'Relative_Humidity': float,\n",
    "                'Mixing_Ratio': float,\n",
    "                'Wind_Direction': float,\n",
    "                'Wind_Speed': float,\n",
    "                'Potential_Temp': float,\n",
    "                'Equivalent_Potential_Temp': float,\n",
    "                'Virtual_Potential_Temp': float,\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe82d65",
   "metadata": {},
   "source": [
    "# Patch Dataset\n",
    "---\n",
    "Populate missing or incomplete data entries with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "85e39ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# PATCH DATASET\n",
    "# Function to create empty entries for missing data\n",
    "# This results in a data entries for each expected date/location/pressure reading\n",
    "# -------------------------------------------------------------------------------\n",
    "def patch_dataset(data, start_date=date(2020, 1, 1), end_date=datetime.today().date()):\n",
    "\n",
    "    dates = daterange(start_date, end_date)\n",
    "\n",
    "\n",
    "    stations = ['Quillayute', 'Vernon', 'Port Hardy']\n",
    "    times = {'AM': [0, 11], 'PM': [12, 23]}\n",
    "    pressures = [700, 850]\n",
    "    idx = len(data)\n",
    "    # Itterate through date range\n",
    "    for date in dates:\n",
    "        for station in stations:\n",
    "            for t, time in times.items():\n",
    "                for pressure in pressures:\n",
    "                    # If data is missing, add empty values\n",
    "                    if data[(data['Date'] == date.strftime(\"%Y-%m-%d\")) & (data['Station'] == station) & (data['Time'] == t) & (data['Pressure'] == pressure)].empty:\n",
    "                        #print(\"Missing data for {}:\\n\\tDate:{}\\n\\tPressure:{}\\n\".format(station, date.strftime(\"%Y-%m-%d\"), pressure))\n",
    "                        new_row = {\n",
    "                            'Date': date.strftime(\"%Y-%m-%d\"), \n",
    "                            'Station': station, \n",
    "                            'Time': t,\n",
    "                            'Pressure': pressure, \n",
    "                            'Height': np.nan,\n",
    "                            'Temp': np.nan,\n",
    "                            'Dew_Point': np.nan,\n",
    "                            'Relative_Humidity': np.nan,\n",
    "                            'Mixing_Ratio': np.nan,\n",
    "                            'Wind_Direction': np.nan,\n",
    "                            'Wind_Speed': np.nan,\n",
    "                            'Potential_Temp': np.nan,\n",
    "                            'Equivalent_Potential_Temp': np.nan,\n",
    "                            'Virtual_Potential_Temp': np.nan}\n",
    "\n",
    "                        #data.loc[idx] = new_row\n",
    "                        data.loc[idx] = [date.strftime(\"%Y-%m-%d\"), station, t, pressure, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "                        idx += 1\n",
    "                        #data.loc[-1] = new_row\n",
    "\n",
    "    # Return complete data\n",
    "    print(\"Patching Complete\")\n",
    "    return data.sort_values(by=['Date', 'Station', 'Pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afc54a",
   "metadata": {},
   "source": [
    "# Upload Complete Dataset\n",
    "---\n",
    "Upload a complete dataset to the GoogleCloudStorage bucket and create daily backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "063f0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# UPLOAD WEATHER DATA\n",
    "# Saves data to cloud bucket\n",
    "# -------------------------------------------------------------------------------\n",
    "def upload_weather_data(data):\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # SAVE DATA TO GOOGLE CLOUD\n",
    "    print(\"Saving data to Google Cloud Bucket\")\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \n",
    "\n",
    "    # Connect to Google Cloud Storage\n",
    "    # -------------------------------\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Open Bucket\n",
    "    # -----------\n",
    "    bucket_name = 'weather_aurorabc'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Save today's backup\n",
    "    # -------------------\n",
    "    mybuffer = io.BytesIO()\n",
    "    data.to_csv(mybuffer, index=False)\n",
    "\n",
    "    blob = bucket.blob('backups/weather_ampm-{}.csv'.format(datetime.today().date()))\n",
    "    blob.upload_from_string(mybuffer.getvalue())\n",
    "\n",
    "    # Update weather.csv\n",
    "    # ------------------\n",
    "    blob = bucket.blob('weather_ampm.csv')\n",
    "    blob.upload_from_string(mybuffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb279db5",
   "metadata": {},
   "source": [
    "---\n",
    "# Main\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + \n",
    "# ----- ----- ----- -----          START         ----- ----- ----- -----\n",
    "#  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def main():\n",
    "    # Import existing weather data from Google Cloud\n",
    "    # ----------------------------------------------\n",
    "    weather_data = import_weather_data()\n",
    "    \n",
    "    \n",
    "    # Find missing or 'nan' values within current dataset\n",
    "    # ---------------------------------------------------\n",
    "    missing_data = find_missing_vals(weather_data)\n",
    "    \n",
    "    \n",
    "    # Scrape weather balloons for new data within defined date range\n",
    "    # --------------------------------------------------------------\n",
    "    new_data = scrape_data(missing_data[-30:])\n",
    "    \n",
    "    # Handle NaN values and standardize data types\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    new_data.dropna(subset=['Temp', 'Wind_Speed'])\n",
    "    new_data = standardize_dtypes(new_data)\n",
    "    \n",
    "    # Convert wind speed from Knots to KM/H\n",
    "    # -------------------------------------\n",
    "    new_data['Wind_Speed'] = new_data['Wind_Speed'].multiply(1.852)\n",
    "\n",
    "    \n",
    "    # Combine existing weather data with newly extracted data\n",
    "    # -------------------------------------------------------\n",
    "    complete_data = pd.concat([weather_data, new_data])\n",
    "    complete_data = complete_data.sort_values(by=['Date', 'Station', 'Time', 'Pressure'])\n",
    "    \n",
    "    # Patch dataset with NaN values for any missing fields\n",
    "    # ----------------------------------------------------\n",
    "    final_data = patch_dataset(complete_data)\n",
    "    final_data = final_data.sort_values(by=['Date', 'Station', 'Time', 'Pressure'])\n",
    "    \n",
    "    \n",
    "    # Upload clean data to bucket\n",
    "    # ---------------------------\n",
    "    upload_weather_data(complete_data)\n",
    "    \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + \n",
    "# ----- ----- ----- -----           END          ----- ----- ----- -----\n",
    "#  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfbe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915798c",
   "metadata": {},
   "source": [
    "---\n",
    "# END\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
